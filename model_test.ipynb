{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('__Python VERSION: ', sys.version)\n",
    "print('__PyTorch VERSION: ', torch.__version__)\n",
    "print('__CUDA VERSION', )\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "print('Available devices ', torch.cuda.device_count())\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is avaliable on your system\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import augmentations as A\n",
    "\n",
    "# Define the transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "# Define a list of transformations\n",
    "augment_tranmforms = [A.Resize((256, 256)),\n",
    "                    A.RandomHorizontalFlip(),\n",
    "                    A.RandomRotation(degrees=(-35, 35)),\n",
    "                    A.RandomCropWithProbability(220, 0.5),\n",
    "                    A.ToTensor()]\n",
    "\n",
    "# Instanciate the Compose class with the list of transformations\n",
    "augment_transforms = A.Compose(augment_tranmforms)\n",
    "\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\archive'\n",
    "\n",
    "# Create transformed train dataset\n",
    "training_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=data_transforms, target_transform=data_transforms)\n",
    "\n",
    "# Create augmented train dataset\n",
    "augmented_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=data_transforms, target_transform=augment_transforms)\n",
    "\n",
    "# Create transformed train dataset\n",
    "validation_dataset = Cityscapes(dataset_path, split='val', mode='fine', target_type='semantic', transform=data_transforms, target_transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 15))\n",
    "\n",
    "for i in range(4):\n",
    "    # TO DO: spec the fig size\n",
    "    img, lbl = training_dataset[i]\n",
    "\n",
    "    img_np = img.permute(1, 2, 0)\n",
    "    lbl_np = lbl.permute(1, 2, 0)\n",
    "\n",
    "    plt.subplot(4, 2, 2 * i + 1)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(f'RGB Image {i+1}')\n",
    "\n",
    "    plt.subplot(4, 2, 2 * i + 2)\n",
    "    plt.imshow(lbl_np)  # Adjust the colormap as needed\n",
    "    plt.title(f'Semantic Segmentation Label {i+1}')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "# plt.tight_layout()\n",
    "    \n",
    "# Save a figure to a PNG format file\n",
    "# plt.savefig('Cityspace-test-vis.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "\n",
    "# Combine the datasets\n",
    "combined_dataset = ConcatDataset([training_dataset, augmented_dataset])\n",
    "\n",
    "# Determine the lengths of the training and validation sets\n",
    "total_size = len(combined_dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "val_size = total_size - train_size  # 20% for validation\n",
    "\n",
    "# Split the datasets\n",
    "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
    "\n",
    "# Create the dataloaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, \n",
    "#                                 pin_memory=True if torch.cuda.is_available() else False)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, \n",
    "#                                 pin_memory=True if torch.cuda.is_available() else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create training and validation dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=10, shuffle=True, num_workers=2,\n",
    "                                            pin_memory=True if torch.cuda.is_available() else False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=10, shuffle=True, num_workers=2,\n",
    "                                            pin_memory=True if torch.cuda.is_available() else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataset\n",
    "# Create transformed train dataset\n",
    "test_dataset = Cityscapes(dataset_path, split='test', mode='fine', target_type='semantic', transform=data_transforms, target_transform=data_transforms)\n",
    "# Create a test data loader\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=True, num_workers=2,\n",
    "                                            pin_memory=True if torch.cuda.is_available() else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from model_vis import visualize_segmentation_cityscapes\n",
    "\n",
    "# Instanciate Model()\n",
    "eval_model = Model()\n",
    "\n",
    "# Specify the checkpoint file path\n",
    "checkpoint_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\model_checkpoints\\\\model_e60_26Mar.pth'\n",
    "device = torch.device('cpu') \n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "eval_model.load_state_dict(checkpoint)\n",
    "\n",
    "# Call the function with the checkpoint path\n",
    "visualize_segmentation_cityscapes(eval_model, train_loader, num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import Model\n",
    "# import model_executables as mex\n",
    "\n",
    "# # Instanciate the model\n",
    "# UNet_model = Model()\n",
    "\n",
    "# # Move the model to the GPU if avaliable\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# UNet_model = UNet_model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "# optimizer = optim.Adam(UNet_model.parameters(), lr=0.01)\n",
    "\n",
    "# # Train the instanciated model\n",
    "# mex.train_model(UNet_model, train_loader, val_loader, num_epochs=2, patience=3, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MSU_Net\n",
    "import model_executables as mex\n",
    "\n",
    "# Instanciate the model\n",
    "UNet_model = MSU_Net()\n",
    "\n",
    "# Move the model to the GPU if avaliable\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "UNet_model = UNet_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = optim.Adam(UNet_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the instanciated model\n",
    "mex.train_model(UNet_model, train_loader, val_loader, num_epochs=2, patience=3, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the checkpoint file and saving a new one with soely the model.state_dict\n",
    "\n",
    "# Instanciate Model()\n",
    "# model = Model()\n",
    "\n",
    "# Specify the checkpoint file path\n",
    "# checkpoint_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\model_checkpoints\\\\model_checkpoint_epoch_50.pth'\n",
    "# device = torch.device('cpu') \n",
    "\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# torch.save(model.state_dict(), 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\model_checkpoints\\\\model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
