{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('__Python VERSION: ', sys.version)\n",
    "print('__PyTorch VERSION: ', torch.__version__)\n",
    "print('__CUDA VERSION', )\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "print('Available devices ', torch.cuda.device_count())\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is avaliable on your system\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import augmentations as A\n",
    "\n",
    "# Define the transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "# Define a list of transformations\n",
    "augment_tranmforms = [A.Resize((256, 256)),\n",
    "                    A.RandomHorizontalFlip(),\n",
    "                    A.RandomRotation(degrees=(-35, 35)),\n",
    "                    A.RandomCropWithProbability(220, 0.5),\n",
    "                    A.ToTensor()]\n",
    "\n",
    "# Instanciate the Compose class with the list of transformations\n",
    "augment_transforms = A.Compose(augment_tranmforms)\n",
    "\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\archive'\n",
    "\n",
    "# Create transformed train dataset\n",
    "training_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=data_transforms, target_transform=data_transforms)\n",
    "\n",
    "# Create augmented train dataset\n",
    "augmented_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=data_transforms, target_transform=augment_transforms)\n",
    "\n",
    "# Create transformed train dataset\n",
    "validation_dataset = Cityscapes(dataset_path, split='val', mode='fine', target_type='semantic', transform=data_transforms, target_transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 15))\n",
    "\n",
    "for i in range(4):\n",
    "    # TO DO: spec the fig size\n",
    "    img, lbl = training_dataset[i]\n",
    "\n",
    "    img_np = img.permute(1, 2, 0)\n",
    "    lbl_np = lbl.permute(1, 2, 0)\n",
    "\n",
    "    plt.subplot(4, 2, 2 * i + 1)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(f'RGB Image {i+1}')\n",
    "\n",
    "    plt.subplot(4, 2, 2 * i + 2)\n",
    "    plt.imshow(lbl_np)  # Adjust the colormap as needed\n",
    "    plt.title(f'Semantic Segmentation Label {i+1}')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "# plt.tight_layout()\n",
    "    \n",
    "# Save a figure to a PNG format file\n",
    "# plt.savefig('Cityspace-test-vis.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "\n",
    "# Combine the datasets\n",
    "combined_dataset = ConcatDataset([training_dataset, augmented_dataset])\n",
    "\n",
    "# Determine the lengths of the training and validation sets\n",
    "total_size = len(combined_dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "val_size = total_size - train_size  # 20% for validation\n",
    "\n",
    "# Split the datasets\n",
    "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
    "\n",
    "# Create the dataloaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, \n",
    "#                                 pin_memory=True if torch.cuda.is_available() else False)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, \n",
    "#                                 pin_memory=True if torch.cuda.is_available() else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create training and validation dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataset\n",
    "# Create transformed train dataset\n",
    "test_dataset = Cityscapes(dataset_path, split='test', mode='fine', target_type='semantic', transform=data_transforms, target_transform=data_transforms)\n",
    "# Create a test data loader\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=True, num_workers=2,\n",
    "#                                             pin_memory=True if torch.cuda.is_available() else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import Model\n",
    "# from models import MSU_Net\n",
    "# from RCNN_UNet import R2U_Net\n",
    "# from model_vis import visualize_segmentation_cityscapes\n",
    "\n",
    "# # Instanciate Model()\n",
    "# eval_model = R2U_Net()\n",
    "\n",
    "# # Specify the checkpoint file path\n",
    "# checkpoint_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\model_checkpoints\\\\model_checkpoint_epoch_50.pth'\n",
    "# device = torch.device('cpu') \n",
    "\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# eval_model.load_state_dict(checkpoint)\n",
    "\n",
    "# # Call the function with the checkpoint path\n",
    "# visualize_segmentation_cityscapes(eval_model, train_loader, num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda3\\envs\\experiments\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  18, 255], device='cuda:0')\n",
      "Focal loss: 2.9515790939331055\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 2.5950803756713867\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 2.3470587730407715\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  18, 255], device='cuda:0')\n",
      "Focal loss: 2.0390779972076416\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 1.4758036136627197\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 1.415526032447815\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15, 255], device='cuda:0')\n",
      "Focal loss: 1.248514175415039\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 1.2708089351654053\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 1.3892827033996582\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 1.5143940448760986\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        255], device='cuda:0')\n",
      "Focal loss: 1.2856014966964722\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 1.0348365306854248\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  17, 255], device='cuda:0')\n",
      "Focal loss: 1.1106712818145752\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  13, 255],\n",
      "       device='cuda:0')\n",
      "Focal loss: 1.1253745555877686\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.7540282607078552\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  16,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.9968441128730774\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  18, 255], device='cuda:0')\n",
      "Focal loss: 1.2172858715057373\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.8238301277160645\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  13, 255],\n",
      "       device='cuda:0')\n",
      "Focal loss: 0.7927870750427246\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        255], device='cuda:0')\n",
      "Focal loss: 0.892918050289154\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.8084943890571594\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  18, 255], device='cuda:0')\n",
      "Focal loss: 1.1125097274780273\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.8687068223953247\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.7250704169273376\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         15,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.6037453413009644\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.8758718371391296\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        255], device='cuda:0')\n",
      "Focal loss: 0.8373591899871826\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  18, 255], device='cuda:0')\n",
      "Focal loss: 0.9461251497268677\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  13,  14,\n",
      "         15, 255], device='cuda:0')\n",
      "Focal loss: 0.9275336265563965\n",
      "Targets: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        255], device='cuda:0')\n",
      "Focal loss: 0.6580889225006104\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "import model_executables as mex\n",
    "from focal import FocalLoss\n",
    "\n",
    "# Instanciate the model\n",
    "UNet_model = Model()\n",
    "\n",
    "# Move the model to the GPU if avaliable\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "UNet_model = UNet_model.to(device)\n",
    "\n",
    "criterion = FocalLoss(ignore_index=255)\n",
    "optimizer = optim.Adam(UNet_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the instanciated model\n",
    "mex.train_model(UNet_model, train_loader, val_loader, num_epochs=2, patience=3, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import MSU_Net\n",
    "# import model_executables as mex\n",
    "\n",
    "# # Instanciate the model\n",
    "# UNet_model = MSU_Net()\n",
    "\n",
    "# # Move the model to the GPU if avaliable\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# UNet_model = UNet_model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "# optimizer = optim.Adam(UNet_model.parameters(), lr=0.01)\n",
    "\n",
    "# # Train the instanciated model\n",
    "# mex.train_model(UNet_model, train_loader, val_loader, num_epochs=2, patience=3, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from RCNN_UNet import R2U_Net\n",
    "\n",
    "# # Instanciate the model\n",
    "# UNet_model = R2U_Net()\n",
    "\n",
    "# # Move the model to the GPU if avaliable\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# UNet_model = UNet_model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "# optimizer = optim.Adam(UNet_model.parameters(), lr=0.01)\n",
    "\n",
    "# # Train the instanciated model\n",
    "# mex.train_model(UNet_model, train_loader, val_loader, num_epochs=2, patience=3, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the checkpoint file and saving a new one with soely the model.state_dict\n",
    "\n",
    "# Instanciate Model()\n",
    "# model = Model()\n",
    "\n",
    "# Specify the checkpoint file path\n",
    "# checkpoint_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\model_checkpoints\\\\model_checkpoint_epoch_50.pth'\n",
    "# device = torch.device('cpu') \n",
    "\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# torch.save(model.state_dict(), 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\model_checkpoints\\\\model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
