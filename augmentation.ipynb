{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "import augmentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of transformations\n",
    "augment_tranmforms = [A.Resize((256, 256)), # This resize is to get a reference when cropping\n",
    "                    A.RandomHorizontalFlip(),\n",
    "                    A.RandomCropWithProbability(220, 0.9),\n",
    "                    A.RandomRotation(degrees=(-35, 35)),\n",
    "                    A.Resize((256, 256)), # this resize is to make sure that all the output images have intened size\n",
    "                    A.ToTensor()]\n",
    "\n",
    "# Instanciate the Compose class with the list of transformations\n",
    "data_transforms = A.Compose(augment_tranmforms)\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\archive'\n",
    "\n",
    "# Augmenting the images and mask at the same time\n",
    "# Create transformed and AUGMENTED train dataset\n",
    "augment_train_dataset = Cityscapes(dataset_path, split='train', mode='fine',\n",
    "                            target_type='semantic', transforms=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 10 # munber of sampels to visualize\n",
    "# Get the first three samples from the training dataset\n",
    "samples = [augment_train_dataset[i] for i in range(N)]\n",
    "\n",
    "# Create a 3x2 subplot grid\n",
    "fig, axs = plt.subplots(N, 2, figsize=(10, 3*N))\n",
    "\n",
    "for i, (img, mask) in enumerate(samples):\n",
    "    # The images and masks are PyTorch tensors, so we need to convert them to numpy arrays for visualization\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    mask = mask.squeeze().numpy()\n",
    "\n",
    "    # Plot the image in the first column\n",
    "    axs[i, 0].imshow(img)\n",
    "    axs[i, 0].axis('off')\n",
    "    axs[i, 0].set_title('RGB Image -- Size:' + str(img.shape))\n",
    "\n",
    "    # Plot the mask in the second column\n",
    "    axs[i, 1].imshow(mask)\n",
    "    axs[i, 1].axis('off')\n",
    "    axs[i, 1].set_title('Target -- Size:' + str(mask.shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "# Define a list of transformations\n",
    "aug_tranmforms = [A.Resize((256, 256)), # This resize is to get a reference when cropping\n",
    "                    A.RandomHorizontalFlip(),\n",
    "                    A.RandomCropWithProbability(220, 0.9),\n",
    "                    A.RandomRotation(degrees=(-35, 35)),\n",
    "                    A.Resize((256, 256)), # this resize is to make sure that all the output images have intened size\n",
    "                    A.ToTensor()]\n",
    "\n",
    "# Instanciate the Compose class with the list of transformations\n",
    "augment_transforms = A.Compose(aug_tranmforms)\n",
    "\n",
    "# Create transformed train dataset\n",
    "training_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=data_transforms, target_transform=data_transforms)\n",
    "# training_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transforms=data_transforms)\n",
    "\n",
    "print(f\"Number of samples in the training dataset: {len(training_dataset)}\")\n",
    "\n",
    "# Create augmented train dataset\n",
    "# augmented_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=augment_tranmforms, target_transform=augment_transforms)\n",
    "augmented_dataset = Cityscapes(dataset_path, split='train', mode='fine',\n",
    "                            target_type='semantic', transforms=augment_transforms)\n",
    "\n",
    "print(f\"Number of samples in the augmented dataset: {len(augmented_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first item from the dataset\n",
    "item = training_dataset[0]\n",
    "\n",
    "# Print the type of the item\n",
    "print(f'Type: {type(item)}')\n",
    "\n",
    "img, mask = item\n",
    "\n",
    "# If the item is a tuple, print the type and size of its elements\n",
    "if isinstance(item, tuple):\n",
    "    print(f'First element type (image): {type(img)}, size: {img.shape}')\n",
    "    print(f'Second element type (mask): {type(mask)}, size: {mask.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_item= augmented_dataset[0]\n",
    "\n",
    "# Print the type of the item\n",
    "print(f'Type: {type(augment_item)}')\n",
    "\n",
    "augment_img, augment_mask = augment_item\n",
    "\n",
    "# If the item is a tuple, print the type and size of its elements\n",
    "if isinstance(augment_item, tuple):\n",
    "    print(f'First element type (image): {type(augment_img)}, size: {augment_img.shape}')\n",
    "    print(f'Second element type (mask): {type(augment_mask)}, size: {augment_mask.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "\n",
    "# Combine the datasets\n",
    "combined_dataset = ConcatDataset([training_dataset, augmented_dataset])\n",
    "\n",
    "print(f\"Number of samples in the combined dataset: {len(combined_dataset)}\")\n",
    "\n",
    "# Determine the lengths of the training and validation sets\n",
    "total_size = len(combined_dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "val_size = total_size - train_size  # 20% for validation\n",
    "\n",
    "# Shuffle and Split the combined dataset \n",
    "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, \n",
    "                                pin_memory=True if torch.cuda.is_available() else False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, \n",
    "                                pin_memory=True if torch.cuda.is_available() else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheching the content of the dataloader\n",
    "# For the train dataloader\n",
    "for i, (inputs, targets) in enumerate(train_dataloader):\n",
    "    print(f\"In batch {i}, the type of inputs is {type(inputs)} and the type of targets is {type(targets)}\")\n",
    "    break  # We break after the first batch as we just want to know the type\n",
    "\n",
    "# For the validation dataloader\n",
    "for i, (inputs, targets) in enumerate(val_dataloader):\n",
    "    print(f\"In batch {i}, the type of inputs is {type(inputs)} and the type of targets is {type(targets)}\")\n",
    "    break  # We break after the first batch as we just want to know the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 10 # number of sampels\n",
    "\n",
    "# Get the first three samples from the training dataset\n",
    "samples = [train_dataset[i] for i in range(N)]\n",
    "\n",
    "# Create a 3x2 subplot grid\n",
    "fig, axs = plt.subplots(N, 2, figsize=(10, 3*N))\n",
    "\n",
    "for i, (img, mask) in enumerate(samples):\n",
    "    # The images and masks are PyTorch tensors, so we need to convert them to numpy arrays for visualization\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    mask = mask.squeeze().numpy()\n",
    "\n",
    "    # Plot the image in the first column\n",
    "    axs[i, 0].imshow(img)\n",
    "    axs[i, 0].axis('off')\n",
    "    axs[i, 0].set_title('RGB Image')\n",
    "\n",
    "    # Plot the mask in the second column\n",
    "    axs[i, 1].imshow(mask)\n",
    "    axs[i, 1].axis('off')\n",
    "    axs[i, 1].set_title('Semantic Segmentation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=255, log_loss=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.ingore_index = ignore_index\n",
    "        self.log_loss = log_loss\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        smooth = 1.\n",
    "\n",
    "        # Apply softmax to input (model output)\n",
    "        input = torch.softmax(input, dim=1)\n",
    "\n",
    "        dice_loss = 0.\n",
    "\n",
    "        print(input.size)\n",
    "\n",
    "        # print('shape of input: ', input.size(1))\n",
    "        for class_index in range(input.size(1)):\n",
    "            valid = (target != self.ingore_index)\n",
    "            # print(f'shape of valid: {valid}')\n",
    "            # print(f'shape of input: {input.shape}')\n",
    "            # print(f'shape of target: {target.shape}')\n",
    "            input_flat = input[:, class_index, :, :][valid].contiguous().view(-1)\n",
    "            target_flat = (target == class_index)[valid].contiguous().view(-1) # binary target for class_index\n",
    "\n",
    "            intersection = (input_flat * target_flat).sum()\n",
    "\n",
    "            class_loss = 1 - ((2. * intersection + smooth) / (input_flat.sum() + target_flat.sum() + smooth))\n",
    "            dice_loss += class_loss\n",
    "\n",
    "            print(f\"Loss for class {class_index}: {class_loss.item()}\")\n",
    "            print(f\"Aggregated loss: {dice_loss.item()}\")\n",
    "\n",
    "        mean_dice = dice_loss/input.size(1) # average loss over all classes\n",
    "        print(f\"Mean Dice Loss (avg. loss for all classes): {mean_dice.item()}\")\n",
    "\n",
    "        if self.log_loss:\n",
    "            mean_dice = -torch.log(mean_dice)\n",
    "\n",
    "        return mean_dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "import utils\n",
    "\n",
    "import torch.optim as optim\n",
    "import losses as L\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instanciate Model()\n",
    "model = Model()\n",
    "\n",
    "# Specify the checkpoint file path\n",
    "checkpoint_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\model_checkpoints\\\\model_checkpoint_epoch_60_Mar14.pth'\n",
    "device = torch.device('cpu') \n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "criterion = DiceLoss(ignore_index=255, log_loss=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# VALIDATION\n",
    "# Initialize accuracy variables for each epoch\n",
    "total_correct_train, total_samples_train = 0, 0\n",
    "total_correct_val, total_samples_val = 0, 0\n",
    "# TRAINING\n",
    "model.train()\n",
    "running_train_loss = 0.0\n",
    "running_val_loss = 0.0\n",
    "\n",
    "for val_inputs, val_masks in train_dataloader:\n",
    "    # Move inputs and masks to the GPU\n",
    "    val_inputs, val_masks = val_inputs.to(device), val_masks.to(device)\n",
    "    val_outputs = model(val_inputs)\n",
    "    print(val_outputs.shape)\n",
    "    val_masks = (val_masks*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "    val_masks = utils.map_id_to_train_id(val_masks).to(device)\n",
    "    val_loss = criterion(val_outputs, val_masks)\n",
    "    running_val_loss += val_loss.item()\n",
    "    # Compute validation accuracy\n",
    "    _, val_predicted = torch.max(val_outputs, 1)\n",
    "    total_correct_val += (val_predicted == val_masks.long().squeeze()).sum().item()\n",
    "    total_samples_val += val_masks.numel()\n",
    "\n",
    "    break\n",
    "epoch_val_loss = running_val_loss\n",
    "print(f\"Validation epoch loss: {epoch_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the training lood with the validation loader and the augmented dataset\n",
    "# from model import Model\n",
    "# import model_executables as mex\n",
    "# import losses as L\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "\n",
    "# # Instanciate the model\n",
    "# UNet_model = Model()\n",
    "\n",
    "# # Move the model to the GPU if avaliable\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# UNet_model = UNet_model.to(device)\n",
    "\n",
    "# criterion = L.DiceLoss(ignore_index=255, log_loss=True)\n",
    "# optimizer = optim.Adam(UNet_model.parameters(), lr=0.01)\n",
    "\n",
    "# # Train the instanciated model\n",
    "# mex.train_model(UNet_model, train_dataloader, val_dataloader, num_epochs=2, patience=3, criterion=criterion, optimizer=optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
