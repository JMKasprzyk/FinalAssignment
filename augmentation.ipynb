{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "import augmentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of transformations\n",
    "augment_tranmforms = [A.Resize((256, 256)), # This resize is to get a reference when cropping\n",
    "                    A.RandomHorizontalFlip(),\n",
    "                    A.RandomCropWithProbability(220, 0.9),\n",
    "                    A.RandomRotation(degrees=(-35, 35)),\n",
    "                    A.Resize((256, 256)), # this resize is to make sure that all the output images have intened size\n",
    "                    A.ToTensor()]\n",
    "\n",
    "# Instanciate the Compose class with the list of transformations\n",
    "data_transforms = A.Compose(augment_tranmforms)\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\archive'\n",
    "\n",
    "# Augmenting the images and mask at the same time\n",
    "# Create transformed and AUGMENTED train dataset\n",
    "augment_train_dataset = Cityscapes(dataset_path, split='train', mode='fine',\n",
    "                            target_type='semantic', transforms=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 10 # munber of sampels to visualize\n",
    "# Get the first three samples from the training dataset\n",
    "samples = [augment_train_dataset[i] for i in range(N)]\n",
    "\n",
    "# Create a 3x2 subplot grid\n",
    "fig, axs = plt.subplots(N, 2, figsize=(10, 3*N))\n",
    "\n",
    "for i, (img, mask) in enumerate(samples):\n",
    "    # The images and masks are PyTorch tensors, so we need to convert them to numpy arrays for visualization\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    mask = mask.squeeze().numpy()\n",
    "\n",
    "    # Plot the image in the first column\n",
    "    axs[i, 0].imshow(img)\n",
    "    axs[i, 0].axis('off')\n",
    "    axs[i, 0].set_title('RGB Image -- Size:' + str(img.shape))\n",
    "\n",
    "    # Plot the mask in the second column\n",
    "    axs[i, 1].imshow(mask)\n",
    "    axs[i, 1].axis('off')\n",
    "    axs[i, 1].set_title('Target -- Size:' + str(mask.shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "# Define a list of transformations\n",
    "aug_tranmforms = [A.Resize((256, 256)), # This resize is to get a reference when cropping\n",
    "                    A.RandomHorizontalFlip(),\n",
    "                    A.RandomCropWithProbability(220, 0.9),\n",
    "                    A.RandomRotation(degrees=(-35, 35)),\n",
    "                    A.Resize((256, 256)), # this resize is to make sure that all the output images have intened size\n",
    "                    A.ToTensor()]\n",
    "\n",
    "# Instanciate the Compose class with the list of transformations\n",
    "augment_transforms = A.Compose(aug_tranmforms)\n",
    "\n",
    "# Create transformed train dataset\n",
    "training_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=data_transforms, target_transform=data_transforms)\n",
    "# training_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transforms=data_transforms)\n",
    "\n",
    "print(f\"Number of samples in the training dataset: {len(training_dataset)}\")\n",
    "\n",
    "# Create augmented train dataset\n",
    "# augmented_dataset = Cityscapes(dataset_path, split='train', mode='fine', target_type='semantic', transform=augment_tranmforms, target_transform=augment_transforms)\n",
    "augmented_dataset = Cityscapes(dataset_path, split='train', mode='fine',\n",
    "                            target_type='semantic', transforms=augment_transforms)\n",
    "\n",
    "print(f\"Number of samples in the augmented dataset: {len(augmented_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first item from the dataset\n",
    "item = training_dataset[0]\n",
    "\n",
    "# Print the type of the item\n",
    "print(f'Type: {type(item)}')\n",
    "\n",
    "img, mask = item\n",
    "\n",
    "# If the item is a tuple, print the type and size of its elements\n",
    "if isinstance(item, tuple):\n",
    "    print(f'First element type (image): {type(img)}, size: {img.shape}')\n",
    "    print(f'Second element type (mask): {type(mask)}, size: {mask.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_item= augmented_dataset[0]\n",
    "\n",
    "# Print the type of the item\n",
    "print(f'Type: {type(augment_item)}')\n",
    "\n",
    "augment_img, augment_mask = augment_item\n",
    "\n",
    "# If the item is a tuple, print the type and size of its elements\n",
    "if isinstance(augment_item, tuple):\n",
    "    print(f'First element type (image): {type(augment_img)}, size: {augment_img.shape}')\n",
    "    print(f'Second element type (mask): {type(augment_mask)}, size: {augment_mask.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "\n",
    "# Combine the datasets\n",
    "combined_dataset = ConcatDataset([training_dataset, augmented_dataset])\n",
    "\n",
    "print(f\"Number of samples in the combined dataset: {len(combined_dataset)}\")\n",
    "\n",
    "# Determine the lengths of the training and validation sets\n",
    "total_size = len(combined_dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "val_size = total_size - train_size  # 20% for validation\n",
    "\n",
    "# Shuffle and Split the combined dataset \n",
    "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, \n",
    "                                pin_memory=True if torch.cuda.is_available() else False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, \n",
    "                                pin_memory=True if torch.cuda.is_available() else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheching the content of the dataloader\n",
    "# For the train dataloader\n",
    "for i, (inputs, targets) in enumerate(train_dataloader):\n",
    "    print(f\"In batch {i}, the type of inputs is {type(inputs)} and the type of targets is {type(targets)}\")\n",
    "    break  # We break after the first batch as we just want to know the type\n",
    "\n",
    "# For the validation dataloader\n",
    "for i, (inputs, targets) in enumerate(val_dataloader):\n",
    "    print(f\"In batch {i}, the type of inputs is {type(inputs)} and the type of targets is {type(targets)}\")\n",
    "    break  # We break after the first batch as we just want to know the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 10 # number of sampels\n",
    "\n",
    "# Get the first three samples from the training dataset\n",
    "samples = [train_dataset[i] for i in range(N)]\n",
    "\n",
    "# Create a 3x2 subplot grid\n",
    "fig, axs = plt.subplots(N, 2, figsize=(10, 3*N))\n",
    "\n",
    "for i, (img, mask) in enumerate(samples):\n",
    "    # The images and masks are PyTorch tensors, so we need to convert them to numpy arrays for visualization\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    mask = mask.squeeze().numpy()\n",
    "\n",
    "    # Plot the image in the first column\n",
    "    axs[i, 0].imshow(img)\n",
    "    axs[i, 0].axis('off')\n",
    "    axs[i, 0].set_title('RGB Image')\n",
    "\n",
    "    # Plot the mask in the second column\n",
    "    axs[i, 1].imshow(mask)\n",
    "    axs[i, 1].axis('off')\n",
    "    axs[i, 1].set_title('Semantic Segmentation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import Dice\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=255, log_loss=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.ingore_index = ignore_index\n",
    "        self.log_loss = log_loss\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        smooth = 1.\n",
    "\n",
    "        # Apply softmax to input (model output)\n",
    "        input = torch.softmax(input, dim=1)\n",
    "\n",
    "        dice_loss = 0.\n",
    "\n",
    "        print(input.size)\n",
    "\n",
    "        unique_classes = set()\n",
    "\n",
    "        # print('shape of input: ', input.size(1))\n",
    "        for class_index in range(input.size(1)):\n",
    "            valid = (target != self.ingore_index)\n",
    "            # print(f'shape of valid: {valid}')\n",
    "            # print(f'shape of input: {input.shape}')\n",
    "            # print(f'shape of target: {target.shape}')\n",
    "            input_flat = input[:, class_index, :, :][valid].contiguous().view(-1)\n",
    "            target_flat = (target == class_index)[valid].contiguous().view(-1) # binary target for class_index\n",
    "\n",
    "            unique_target_classes = torch.unique(target_flat)\n",
    "            if 1 in unique_target_classes:\n",
    "                unique_classes.add(class_index)\n",
    "\n",
    "            intersection = (input_flat * target_flat).sum()\n",
    "\n",
    "            class_loss = 1 - ((2. * intersection + smooth) / (input_flat.sum() + target_flat.sum() + smooth))\n",
    "            dice_loss += class_loss\n",
    "\n",
    "            print(f\"Loss for class {class_index}: {class_loss.item()}\")\n",
    "            print(f\"Aggregated loss: {dice_loss.item()}\")\n",
    "\n",
    "        mean_dice = dice_loss/input.size(1) # average loss over all classes\n",
    "        print(f\"Mean Dice Loss (avg. loss for all classes): {mean_dice.item()}\")\n",
    "        print(f\"Number of unique classes: {len(unique_classes)}\")\n",
    "\n",
    "        if self.log_loss:\n",
    "            mean_dice = -torch.log(mean_dice)\n",
    "\n",
    "        return mean_dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 34, 256, 256])\n",
      "<built-in method size of Tensor object at 0x0000015BE9A72BD0>\n",
      "Loss for class 0: 0.039670705795288086\n",
      "Aggregated loss: 0.039670705795288086\n",
      "Loss for class 1: 0.3547484874725342\n",
      "Aggregated loss: 0.39441919326782227\n",
      "Loss for class 2: 0.438998818397522\n",
      "Aggregated loss: 0.8334180116653442\n",
      "Loss for class 3: 0.9960727095603943\n",
      "Aggregated loss: 1.8294906616210938\n",
      "Loss for class 4: 0.995869517326355\n",
      "Aggregated loss: 2.8253602981567383\n",
      "Loss for class 5: 0.7684552073478699\n",
      "Aggregated loss: 3.593815565109253\n",
      "Loss for class 6: 0.6537380814552307\n",
      "Aggregated loss: 4.247553825378418\n",
      "Loss for class 7: 0.5735960602760315\n",
      "Aggregated loss: 4.821149826049805\n",
      "Loss for class 8: 0.16935813426971436\n",
      "Aggregated loss: 4.990508079528809\n",
      "Loss for class 9: 0.9912495613098145\n",
      "Aggregated loss: 5.981757640838623\n",
      "Loss for class 10: 0.9993491172790527\n",
      "Aggregated loss: 6.981106758117676\n",
      "Loss for class 11: 0.8233156800270081\n",
      "Aggregated loss: 7.804422378540039\n",
      "Loss for class 12: 0.9972438812255859\n",
      "Aggregated loss: 8.801666259765625\n",
      "Loss for class 13: 0.06239449977874756\n",
      "Aggregated loss: 8.864060401916504\n",
      "Loss for class 14: 0.8362276554107666\n",
      "Aggregated loss: 9.700287818908691\n",
      "Loss for class 15: 0.982462465763092\n",
      "Aggregated loss: 10.682750701904297\n",
      "Loss for class 16: 0.9981271624565125\n",
      "Aggregated loss: 11.680877685546875\n",
      "Loss for class 17: 0.9656504392623901\n",
      "Aggregated loss: 12.646528244018555\n",
      "Loss for class 18: 0.9636721611022949\n",
      "Aggregated loss: 13.610200881958008\n",
      "Loss for class 19: 0.048867225646972656\n",
      "Aggregated loss: 13.65906810760498\n",
      "Loss for class 20: 0.1157921552658081\n",
      "Aggregated loss: 13.774860382080078\n",
      "Loss for class 21: 0.04842853546142578\n",
      "Aggregated loss: 13.823288917541504\n",
      "Loss for class 22: 0.05479240417480469\n",
      "Aggregated loss: 13.878081321716309\n",
      "Loss for class 23: 0.05208587646484375\n",
      "Aggregated loss: 13.930167198181152\n",
      "Loss for class 24: 0.05673283338546753\n",
      "Aggregated loss: 13.986900329589844\n",
      "Loss for class 25: 0.0481799840927124\n",
      "Aggregated loss: 14.035079956054688\n",
      "Loss for class 26: 0.08005434274673462\n",
      "Aggregated loss: 14.115134239196777\n",
      "Loss for class 27: 0.03951841592788696\n",
      "Aggregated loss: 14.15465259552002\n",
      "Loss for class 28: 0.05379873514175415\n",
      "Aggregated loss: 14.208451271057129\n",
      "Loss for class 29: 0.061808645725250244\n",
      "Aggregated loss: 14.270259857177734\n",
      "Loss for class 30: 0.055593907833099365\n",
      "Aggregated loss: 14.32585334777832\n",
      "Loss for class 31: 0.0437280535697937\n",
      "Aggregated loss: 14.36958122253418\n",
      "Loss for class 32: 0.059215664863586426\n",
      "Aggregated loss: 14.428796768188477\n",
      "Loss for class 33: 0.06270182132720947\n",
      "Aggregated loss: 14.491498947143555\n",
      "Mean Dice Loss (avg. loss for all classes): 0.426220566034317\n",
      "Number of unique classes: 14\n",
      "Validation epoch loss: 0.8528\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "import utils\n",
    "\n",
    "import torch.optim as optim\n",
    "import losses as L\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instanciate Model()\n",
    "model = Model()\n",
    "\n",
    "# Specify the checkpoint file path\n",
    "checkpoint_path = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\TUe\\\\AIES\\\\Q3\\\\5LSM0-Neural-networks-for-computer-vision\\\\FinalAssignment\\\\model_checkpoints\\\\model_checkpoint_epoch_60_Mar14.pth'\n",
    "device = torch.device('cpu') \n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "criterion = DiceLoss(ignore_index=255, log_loss=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# VALIDATION\n",
    "# Initialize accuracy variables for each epoch\n",
    "total_correct_train, total_samples_train = 0, 0\n",
    "total_correct_val, total_samples_val = 0, 0\n",
    "# TRAINING\n",
    "model.train()\n",
    "running_train_loss = 0.0\n",
    "running_val_loss = 0.0\n",
    "\n",
    "for val_inputs, val_masks in train_dataloader:\n",
    "    # Move inputs and masks to the GPU\n",
    "    val_inputs, val_masks = val_inputs.to(device), val_masks.to(device)\n",
    "    val_outputs = model(val_inputs)\n",
    "    print(val_outputs.shape)\n",
    "    val_masks = (val_masks*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "    val_masks = utils.map_id_to_train_id(val_masks).to(device)\n",
    "    val_loss = criterion(val_outputs, val_masks)\n",
    "    running_val_loss += val_loss.item()\n",
    "    # Compute validation accuracy\n",
    "    _, val_predicted = torch.max(val_outputs, 1)\n",
    "    total_correct_val += (val_predicted == val_masks.long().squeeze()).sum().item()\n",
    "    total_samples_val += val_masks.numel()\n",
    "\n",
    "    break\n",
    "epoch_val_loss = running_val_loss\n",
    "print(f\"Validation epoch loss: {epoch_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the training lood with the validation loader and the augmented dataset\n",
    "# from model import Model\n",
    "# import model_executables as mex\n",
    "# import losses as L\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "\n",
    "# # Instanciate the model\n",
    "# UNet_model = Model()\n",
    "\n",
    "# # Move the model to the GPU if avaliable\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# UNet_model = UNet_model.to(device)\n",
    "\n",
    "# criterion = L.DiceLoss(ignore_index=255, log_loss=True)\n",
    "# optimizer = optim.Adam(UNet_model.parameters(), lr=0.01)\n",
    "\n",
    "# # Train the instanciated model\n",
    "# mex.train_model(UNet_model, train_dataloader, val_dataloader, num_epochs=2, patience=3, criterion=criterion, optimizer=optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
